{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstructing Xarray Datasets from Model Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "This notebook addresses the process of reconstructing an `xarray.DataArray` from the output of a machine learning model. While the previous notebook focused on generating batches from `xarray` objects, this guide details the reverse process: assembling model outputs back into a coherent, labeled `xarray` object. This is a common requirement in scientific machine learning workflows, where the model output needs to be analyzed in its original spatial or temporal context.\n",
    "\n",
    "We will examine a function that reassembles model outputs, including a detailed look at how an internal API of `xbatcher` can be used to map batch outputs back to their original coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Intro to Xarray](https://tutorial.xarray.dev/overview/xarray-in-45-min.html) | Necessary | Array indexing |\n",
    "| [Loading Batches from Xarray](https://projectpythia.org/xbatcher-deep-learning/notebooks/xbatcher-dataloading/) | Helpful | PyTorch DataLoader API |\n",
    "| [PyTorch fundamentals](https://docs.pytorch.org/tutorials/beginner/basics/intro.html) | Helpful | Model training loop |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch\n",
    "import xbatcher\n",
    "from xbatcher.loaders.torch import MapDataset\n",
    "from typing import Literal\n",
    "\n",
    "from dummy_models import ExpandAlongAxis",
    "from functions import predict_on_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Data, Batches, and a Dummy Model\n",
    "\n",
    "We will begin by creating a sample `xarray.DataArray` and a `BatchGenerator`. We will also instantiate a dummy model that transforms the data, simulating a common machine learning scenario where the output dimensions differ from the input dimensions (e.g., super-resolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = xr.DataArray(\n",
    "    data=np.random.rand(50, 40).astype(np.float32),\n",
    "    dims=(\"x\", \"y\"),\n",
    "    coords={\"x\": np.arange(50), \"y\": np.arange(40)},\n",
    ")\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the `BatchGenerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgen = xbatcher.BatchGenerator(da, input_dims={\"x\": 10, \"y\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model, we will use `ExpandAlongAxis` from `dummy_models.py`. This model upsamples the input along a specified axis, changing the dimensions of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model will expand the 'x' dimension by a factor of 2\n",
    "model = ExpandAlongAxis(ax=1, n_repeats=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing the Dataset\n",
    "\n",
    "We will now use the `predict_on_array` function to reconstruct the dataset. The most important part of using this function is correctly specifying the arguments `new_dim`, `core_dim`, and `resample_dim`. These lists all contain dimensions given in `output_tensor_dim` and help us get an idea of how model output compares to the input data. In general:\n",
    "\n",
    " - `new_dim`: Tensor dimensions that do not appear at all in the original xarray object.\n",
    " - `core_dim`: Tensor dimensions that are present in the original xarray object, but are not used for batch generation. We assume that all elements of this dimension in the xarray object flow through the model to the output. Coordinates are simply copied from the original xarray object.\n",
    " - `resample_dim`: Tensor dimensions that are present in the original xarray object *and* used for batch generation. These dimensions are allowed to change size (but see below note) and coordinates are resampled accordingly in the reconstructed array.\n",
    "\n",
    "Let's apply these rules to our present example. The batch generator creates tensors of size `(x=10, y=10)` and the dummy model makes tensors of size `(x=20, y=10)`. In this case, all tensor dimensions are present in the original data array and are used for batch generation. Therefore, both `x` and `y` go in `resample_dim`. Now that all tensor dimensions are accounted for, we can simply leave `new_dim` and `core_dim` as empty lists.\n",
    "\n",
    ":::{admonition} Changing dimension sizes\n",
    ":class: tip\n",
    "\n",
    "Dimensions in `resample_dim` must upsample or downsample by a factor that implies an integer size in the output data array. For example, in this example we generated tensors of size `(x=10, y=10)` and the model generates tensors of size `(x=20, y=10)`. This implies that we are *upsampling* `x` by a factor of 2. The original data array has size of 50 in the `x` dimension, and `50 * 2 = 100` is an integer, so this is allowed.\n",
    "\n",
    "What if, for some reason, the batch generator made tensors of size `(x=7, y=7)` and the model generated tensors of size `(x=10, y=10)`? The resampling factor becomes `10/7`, and the implied output data array size is `(50 * 10/7) = 72.43`. This is not an integer, so `predict_on_array` will throw an error.\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dataset = MapDataset(bgen)\n",
    "reconstructed_da = predict_on_array(\n",
    "    dataset=map_dataset,\n",
    "    model=model,\n",
    "    output_tensor_dim={\"x\": 20, \"y\": 10}, # The model doubles the x-dimension\n",
    "    new_dim=[],\n",
    "    core_dim=[],\n",
    "    resample_dim=[\"x\", \"y\"],\n",
    "    batch_size=4,\n",
    "    progress_bar=False\n",
    ")\n",
    "reconstructed_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstructed `DataArray` has the upsampled `x` dimension. We can compare its shape to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original shape: {da.shape}\")\n",
    "print(f\"Reconstructed shape: {reconstructed_da.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstructed array has twice the number of elements in the `x` dimension, as expected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookbook-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
