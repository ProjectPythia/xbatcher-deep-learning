{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `xbatcher` to train an autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In previous notebooks we have demonstrated how `xbatcher` converts both toy `xarray` objects into tensors and back again. In this notebook we incorporate these functions in an end-to-end workflow training an autoencoder on an elevation dataset. Once trained, the model is used to reconstruct two datasets:\n",
    "\n",
    " - The overall elevation tile\n",
    " - A data cube of the autoencoder's latent dimension\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook assumes familiarity with `xarray`, `xbatcher`, and `torch`. You don't have to know how autoencoders work - we explain that when necessary.\n",
    "\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Intro to Xarray](https://tutorial.xarray.dev/overview/xarray-in-45-min.html) | Necessary | Array indexing |\n",
    "| [Xbatcher fundamentals](https://projectpythia.org/xbatcher-deep-learning/notebooks/xbatcher-dataloading/) | Necessary | Passing data to models |\n",
    "| [PyTorch fundamentals](https://docs.pytorch.org/tutorials/beginner/basics/intro.html) | Helpful | Model training loop |\n",
    "| [Autoencoders](https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/08-deep-autoencoders.html) | Helpful | More information on how autoencoders work.\n",
    "\n",
    "- **Time to learn**: 30 minutes.\n",
    "- **System requirements**:\n",
    "    - Windows users may hit an import error on `rioxarray` ([link](https://gis.stackexchange.com/questions/417733/unable-to-import-python-rasterio-package-even-though-it-is-installed)). If that happens, add `import osgeo` above `import rioxarray` and that seems to fix the issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.name == 'nt':\n",
    "    import osgeo\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "# DL stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Geospatial stuff\n",
    "import xarray as xr\n",
    "import xbatcher\n",
    "import rioxarray\n",
    "from xbatcher.loaders.torch import MapDataset\n",
    "\n",
    "# Etc\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Locals\n",
    "import functions\n",
    "import autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by pulling a segment of NASADEM for Washington's Olympic peninsula. The entire DEM is also available on NASA Earthdata and Planetary Computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterio adds a blank edge. Trim these out.\n",
    "dem = rioxarray.open_rasterio(\"../ASTGTMV003_N47W124_dem.tif\")\n",
    "dem = dem.isel(y=slice(0, -1), x=slice(0, -1))\n",
    "dem = (dem - dem.min()) / (dem.max() - dem.min())\n",
    "dem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we rescaled the DEM to be in the range [0, 1]. This modification makes it easier for the autoencoder to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem.isel(band=0).plot.imshow(cmap=\"terrain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training examples\n",
    "Here, we use xbatcher to generate patches of terrain. Even though we did not specify the `band` axis in the `BatchGenerator`, this axis still propagates to the output tensor's second axis. Remember that torch data loaders add a batch dimension, so the tensor's first axis is not relevant to the original xarray object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgen_x = xbatcher.BatchGenerator(\n",
    "    dem,\n",
    "    input_dims=dict(x=32, y=32),\n",
    "    input_overlap=dict(x=16, y=16)\n",
    ")\n",
    "\n",
    "ds = MapDataset(\n",
    "    X_generator=bgen_x\n",
    ")\n",
    "\n",
    "loader = torch.utils.data.DataLoader(ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(iter(loader))\n",
    "\n",
    "print(\"Input tensor shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup\n",
    "\n",
    "Here we instantiate the autoencoder class and verify that the output shape is what we expect. Autoencoders compress input data to a latent space and then reconstruct the input from the compressed representation. Usually, we are interested in the compressed result, but during training we are trying to recreate the input exactly. Therefore, the output tensor should match the shape of the input tensor.\n",
    "\n",
    "Input tensor:\n",
    "\n",
    "<svg width=\"177\" height=\"167\" style=\"stroke:rgb(0,0,0);stroke-width:1\"> <!-- Horizontal lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"27\" y2=\"17\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"10\" y1=\"100\" x2=\"27\" y2=\"117\" style=\"stroke-width:2\" /> <!-- Vertical lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"100\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"117\" style=\"stroke-width:2\" /> <!-- Colored Rectangle -->\n",
    "    <polygon points=\"10.0,0.0 27.578209484906253,17.578209484906253 27.578209484906253,117.57820948490625 10.0,100.0\"\n",
    "        style=\"fill:#ECB172A0;stroke-width:0\" /> <!-- Horizontal lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"110\" y2=\"0\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"27\" y1=\"17\" x2=\"127\" y2=\"17\" style=\"stroke-width:2\" /> <!-- Vertical lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"27\" y2=\"17\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"110\" y1=\"0\" x2=\"127\" y2=\"17\" style=\"stroke-width:2\" /> <!-- Colored Rectangle -->\n",
    "    <polygon points=\"10.0,0.0 110.0,0.0 127.57820948490625,17.578209484906253 27.578209484906253,17.578209484906253\"\n",
    "        style=\"fill:#ECB172A0;stroke-width:0\" /> <!-- Horizontal lines -->\n",
    "    <line x1=\"27\" y1=\"17\" x2=\"127\" y2=\"17\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"27\" y1=\"117\" x2=\"127\" y2=\"117\" style=\"stroke-width:2\" /> <!-- Vertical lines -->\n",
    "    <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"117\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"127\" y1=\"17\" x2=\"127\" y2=\"117\" style=\"stroke-width:2\" /> <!-- Colored Rectangle -->\n",
    "    <polygon\n",
    "        points=\"27.578209484906253,17.578209484906253 127.57820948490625,17.578209484906253 127.57820948490625,117.57820948490625 27.578209484906253,117.57820948490625\"\n",
    "        style=\"fill:#ECB172A0;stroke-width:0\" /> <!-- Text --> <text x=\"77.578209\" y=\"137.578209\" font-size=\"1.0rem\"\n",
    "        font-weight=\"100\" text-anchor=\"middle\">x: 32</text> <text x=\"147.578209\" y=\"67.578209\" font-size=\"1.0rem\"\n",
    "        font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,147.578209,67.578209)\">y: 32</text> <text x=\"8.789105\"\n",
    "        y=\"128.789105\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\"\n",
    "        transform=\"rotate(45,8.789105,128.789105)\">band: 1</text>\n",
    "</svg>\n",
    "\n",
    "Output tensor:\n",
    "\n",
    "<svg width=\"177\" height=\"167\" style=\"stroke:rgb(0,0,0);stroke-width:1\"> <!-- Horizontal lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"27\" y2=\"17\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"10\" y1=\"100\" x2=\"27\" y2=\"117\" style=\"stroke-width:2\" /> <!-- Vertical lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"100\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"117\" style=\"stroke-width:2\" /> <!-- Colored Rectangle -->\n",
    "    <polygon points=\"10.0,0.0 27.578209484906253,17.578209484906253 27.578209484906253,117.57820948490625 10.0,100.0\"\n",
    "        style=\"fill:#ECB172A0;stroke-width:0\" /> <!-- Horizontal lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"110\" y2=\"0\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"27\" y1=\"17\" x2=\"127\" y2=\"17\" style=\"stroke-width:2\" /> <!-- Vertical lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"27\" y2=\"17\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"110\" y1=\"0\" x2=\"127\" y2=\"17\" style=\"stroke-width:2\" /> <!-- Colored Rectangle -->\n",
    "    <polygon points=\"10.0,0.0 110.0,0.0 127.57820948490625,17.578209484906253 27.578209484906253,17.578209484906253\"\n",
    "        style=\"fill:#ECB172A0;stroke-width:0\" /> <!-- Horizontal lines -->\n",
    "    <line x1=\"27\" y1=\"17\" x2=\"127\" y2=\"17\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"27\" y1=\"117\" x2=\"127\" y2=\"117\" style=\"stroke-width:2\" /> <!-- Vertical lines -->\n",
    "    <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"117\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"127\" y1=\"17\" x2=\"127\" y2=\"117\" style=\"stroke-width:2\" /> <!-- Colored Rectangle -->\n",
    "    <polygon\n",
    "        points=\"27.578209484906253,17.578209484906253 127.57820948490625,17.578209484906253 127.57820948490625,117.57820948490625 27.578209484906253,117.57820948490625\"\n",
    "        style=\"fill:#ECB172A0;stroke-width:0\" /> <!-- Text --> <text x=\"77.578209\" y=\"137.578209\" font-size=\"1.0rem\"\n",
    "        font-weight=\"100\" text-anchor=\"middle\">x: 32</text> <text x=\"147.578209\" y=\"67.578209\" font-size=\"1.0rem\"\n",
    "        font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,147.578209,67.578209)\">y: 32</text> <text x=\"8.789105\"\n",
    "        y=\"128.789105\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\"\n",
    "        transform=\"rotate(45,8.789105,128.789105)\">band: 1</text>\n",
    "</svg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array.svg as svg\n",
    "from IPython.display import HTML \n",
    "\n",
    "svg.svg(((50,), (10,), (10,)), size=100).replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = autoencoder.Autoencoder(base_channel_size=32, latent_dim=64, num_input_channels=1, width=32, height=32)\n",
    "opt = m._configure_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = m(X)\n",
    "print(out.shape)\n",
    "assert(out.shape == X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "We aren't using pytorch-lightning and load a pre-trained model here to keep the notebook environment lean. For your project, we highly recommend using a framework to abstract away much of the boilerplate code below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    last_loss = 0.\n",
    "    running_loss = 0.\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(loader)):\n",
    "        # Zero your gradients for every batch!\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = m(batch)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = m._get_reconstruction_loss(batch, outputs)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        opt.step()\n",
    "\n",
    "    return running_loss / len(loader)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_epochs = 5\n",
    "for i_epoch in range(n_epochs):\n",
    "    loss = train_one_epoch(i_epoch)\n",
    "    print(f\"Epoch {i_epoch+1:>3}: {loss:.3e}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save(m.state_dict(), \"../autoencoder.torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{danger}\n",
    "This model is certainly overfitted. For brevity we have omitted a validation dataset, which is essential for building models that generalize well on unseen data.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.load_state_dict(torch.load(\"../autoencoder.torch\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.eval()\n",
    "n_examples = 4\n",
    "inputs = next(iter(loader))\n",
    "outputs = m(inputs)\n",
    "\n",
    "inputs = inputs.detach().cpu().numpy()\n",
    "outputs = outputs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, n_examples)\n",
    "\n",
    "for i_col in range(n_examples):\n",
    "    axes[0, i_col].imshow(inputs[i_col, 0, ...])\n",
    "    axes[1, i_col].imshow(outputs[i_col, 0, ...])\n",
    "\n",
    "for a in axes.flat:\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])\n",
    "\n",
    "axes[0, 0].set_ylabel(\"Original patch\")\n",
    "axes[1, 0].set_ylabel(\"Reconstruction\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction 1: Getting the full array back\n",
    "\n",
    "Suppose we would like to evaluate how the autoencoder does on reconstructing the entire terrain patch by combining outputs across all input patches. To do so we can use the `predict_on_array` function described in the previous notebook. Our model outputs tensors with shape `(band=1, x=32, y=32)`. We need to specify each of these axes in the call to `predict_on_array`. `band` does not change size and is not used by the `BatchGenerator`, so it goes in `core_dim`. Both `x` and `y` are used by the `BatchGenerator`, so although they do not change size they still go in `resample_dim`. That accounts for all tensor axes, so we can leave the `new_dim` argument as an empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_reconst = functions.predict_on_array(\n",
    "    dataset=ds,\n",
    "    model=m,\n",
    "    output_tensor_dim=dict(band=1, y=32, x=32),\n",
    "    new_dim=[],\n",
    "    core_dim=[\"band\"],\n",
    "    resample_dim=[\"x\", \"y\"],\n",
    "    progress_bar=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_reconst.isel(band=0).plot.imshow(cmap=\"terrain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That certainly looks like the original DEM. Let's try plotting the error in the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = (dem_reconst - dem)\n",
    "err.isel(band=0).plot.imshow()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err.plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction 2: Getting the latent dimension\n",
    "\n",
    "A common application of autoencoders is to use the latent dimension for some application. Let's turn our autoencoder's predictions into a data cube. To do so we will modify the batch generator to not have overlapping windows. We also have to slightly clip the size of the input DEM. This is because we are effectively downscaling the spatial axes by a factor of 32. Since `3600 / 32` is not an integer, `predict_on_array` will not know how to rescale the array size. So, we have to clip the DEM to the nearest integer multiple of 32. In this case the nearest multiple is 3584, which we achieve by clipping 8 pixels from each side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgen_no_overlap = xbatcher.BatchGenerator(\n",
    "    dem.isel(x=slice(8, -8), y=slice(8, -8)),\n",
    "    input_dims=dict(x=32, y=32),\n",
    "    input_overlap=dict(x=0, y=0)\n",
    ")\n",
    "\n",
    "ds_no_overlap = MapDataset(\n",
    "    X_generator=bgen_no_overlap\n",
    ")\n",
    "\n",
    "loader = torch.utils.data.DataLoader(ds_no_overlap, batch_size=16, shuffle=True)\n",
    "\n",
    "ex_input = next(iter(loader))\n",
    "\n",
    "# Same as before\n",
    "print(\"Input shape:\", ex_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will write a function that the calls the encoder arm of the autoencoder and adds a fake x and y dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_encoder(x):\n",
    "    return m.encoder(x)[:, None, None, :]\n",
    "\n",
    "ex_output = infer_with_encoder(ex_input)\n",
    "print(\"Output shape:\", ex_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be clear, we started with the usual x/y/band tensor from the input dataset.\n",
    "\n",
    "<svg width=\"177\" height=\"167\" style=\"stroke:rgb(0,0,0);stroke-width:1\"> <!-- Horizontal lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"27\" y2=\"17\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"10\" y1=\"100\" x2=\"27\" y2=\"117\" style=\"stroke-width:2\" /> <!-- Vertical lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"100\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"117\" style=\"stroke-width:2\" /> <!-- Colored Rectangle -->\n",
    "    <polygon points=\"10.0,0.0 27.578209484906253,17.578209484906253 27.578209484906253,117.57820948490625 10.0,100.0\"\n",
    "        style=\"fill:#ECB172A0;stroke-width:0\" /> <!-- Horizontal lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"110\" y2=\"0\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"27\" y1=\"17\" x2=\"127\" y2=\"17\" style=\"stroke-width:2\" /> <!-- Vertical lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"27\" y2=\"17\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"110\" y1=\"0\" x2=\"127\" y2=\"17\" style=\"stroke-width:2\" /> <!-- Colored Rectangle -->\n",
    "    <polygon points=\"10.0,0.0 110.0,0.0 127.57820948490625,17.578209484906253 27.578209484906253,17.578209484906253\"\n",
    "        style=\"fill:#ECB172A0;stroke-width:0\" /> <!-- Horizontal lines -->\n",
    "    <line x1=\"27\" y1=\"17\" x2=\"127\" y2=\"17\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"27\" y1=\"117\" x2=\"127\" y2=\"117\" style=\"stroke-width:2\" /> <!-- Vertical lines -->\n",
    "    <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"117\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"127\" y1=\"17\" x2=\"127\" y2=\"117\" style=\"stroke-width:2\" /> <!-- Colored Rectangle -->\n",
    "    <polygon\n",
    "        points=\"27.578209484906253,17.578209484906253 127.57820948490625,17.578209484906253 127.57820948490625,117.57820948490625 27.578209484906253,117.57820948490625\"\n",
    "        style=\"fill:#ECB172A0;stroke-width:0\" /> <!-- Text --> <text x=\"77.578209\" y=\"137.578209\" font-size=\"1.0rem\"\n",
    "        font-weight=\"100\" text-anchor=\"middle\">x: 32</text> <text x=\"147.578209\" y=\"67.578209\" font-size=\"1.0rem\"\n",
    "        font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,147.578209,67.578209)\">y: 32</text> <text x=\"8.789105\"\n",
    "        y=\"128.789105\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\"\n",
    "        transform=\"rotate(45,8.789105,128.789105)\">band: 1</text>\n",
    "</svg>\n",
    "\n",
    "...and end up with a tensor that has singleton x/y dimensions and a new, 64-element channel dimension.\n",
    "\n",
    "<svg width=\"141\" height=\"131\" style=\"stroke:rgb(0,0,0);stroke-width:1\"> <!-- Horizontal lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"68\" y2=\"58\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"10\" y1=\"23\" x2=\"68\" y2=\"81\" style=\"stroke-width:2\" /> <!-- Vertical lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"23\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"68\" y1=\"58\" x2=\"68\" y2=\"81\" style=\"stroke-width:2\" /> <!-- Colored Rectangle -->\n",
    "    <polygon\n",
    "        points=\"10.0,0.0 68.82352941176471,58.82352941176471 68.82352941176471,81.8863520766876 10.0,23.062822664922887\"\n",
    "        style=\"fill:#ECB172A0;stroke-width:0\" /> <!-- Horizontal lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"33\" y2=\"0\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"68\" y1=\"58\" x2=\"91\" y2=\"58\" style=\"stroke-width:2\" /> <!-- Vertical lines -->\n",
    "    <line x1=\"10\" y1=\"0\" x2=\"68\" y2=\"58\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"33\" y1=\"0\" x2=\"91\" y2=\"58\" style=\"stroke-width:2\" /> <!-- Colored Rectangle -->\n",
    "    <polygon\n",
    "        points=\"10.0,0.0 33.06282266492289,0.0 91.8863520766876,58.82352941176471 68.82352941176471,58.82352941176471\"\n",
    "        style=\"fill:#ECB172A0;stroke-width:0\" /> <!-- Horizontal lines -->\n",
    "    <line x1=\"68\" y1=\"58\" x2=\"91\" y2=\"58\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"68\" y1=\"81\" x2=\"91\" y2=\"81\" style=\"stroke-width:2\" /> <!-- Vertical lines -->\n",
    "    <line x1=\"68\" y1=\"58\" x2=\"68\" y2=\"81\" style=\"stroke-width:2\" />\n",
    "    <line x1=\"91\" y1=\"58\" x2=\"91\" y2=\"81\" style=\"stroke-width:2\" /> <!-- Colored Rectangle -->\n",
    "    <polygon\n",
    "        points=\"68.82352941176471,58.82352941176471 91.8863520766876,58.82352941176471 91.8863520766876,81.8863520766876 68.82352941176471,81.8863520766876\"\n",
    "        style=\"fill:#ECB172A0;stroke-width:0\" /> <!-- Text --> <text x=\"80.354941\" y=\"101.886352\" font-size=\"1.0rem\"\n",
    "        font-weight=\"100\" text-anchor=\"middle\">y: 1</text> <text x=\"111.886352\" y=\"70.354941\" font-size=\"1.0rem\"\n",
    "        font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,111.886352,70.354941)\">x: 1</text> <text x=\"29.411765\"\n",
    "        y=\"72.474587\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\"\n",
    "        transform=\"rotate(45,29.411765,72.474587)\">channel: 64</text>\n",
    "</svg>\n",
    "\n",
    "We can go through the same process as before to see how we put together the `predict_on_array` call. Both the `x` and `y` dimensions change size and are used by the batch generator, so they go in `resample_dims`. The remaining dimension, `channel`, is a new dimension and goes in the `new_dim` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim_cube = functions.predict_on_array(\n",
    "    dataset=ds_no_overlap,\n",
    "    model=infer_with_encoder,\n",
    "    output_tensor_dim=dict(y=1, x=1, channel=64),\n",
    "    new_dim=[\"channel\"],\n",
    "    core_dim=[],\n",
    "    resample_dim=[\"x\", \"y\"],\n",
    "    progress_bar=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that despite substantially re-arranging the input `DataArray`, we have retained the coordinate information at a resampled resolution.\n",
    "\n",
    "If we simply sum the output over the channel dimension, we see that the encoder clearly distinguishes between upland and lowland areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim_cube.sum(dim=\"channel\").plot.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final demonstration of this workflow, let's compute the cosine similarity of each of the below pixels with the latent encoding of [Mt. Olympus](https://en.wikipedia.org/wiki/Mount_Olympus_(Washington))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olympus = dict(x=-123.7066, y=47.7998)\n",
    "olympus_latent = latent_dim_cube.sel(**olympus, method=\"nearest\")\n",
    "olympus_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_cosine_similarity(x, y):\n",
    "    return np.dot(x, y)/(norm(x)*norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olympus_similarity = xr.apply_ufunc(\n",
    "    numpy_cosine_similarity,\n",
    "    latent_dim_cube,\n",
    "    input_core_dims = [[\"channel\"]],\n",
    "    output_core_dims = [[]],\n",
    "    vectorize=True,\n",
    "    kwargs=dict(y=olympus_latent.data)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olympus_similarity.plot.imshow()\n",
    "plt.scatter(olympus[\"x\"], olympus[\"y\"], marker=\"*\", c=\"purple\", edgecolor=\"black\", s=200)\n",
    "plt.title(\"Cosine similarity with Mt. Olympus, WA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can identify foothills with similar topography to Grisdale, WA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grisdale = dict(y=47.356625678465925, x=-123.61183314426664)\n",
    "grisdale_latent = latent_dim_cube.sel(**grisdale, method=\"nearest\")\n",
    "\n",
    "grisdale_similarity = xr.apply_ufunc(\n",
    "    numpy_cosine_similarity,\n",
    "    latent_dim_cube,\n",
    "    input_core_dims = [[\"channel\"]],\n",
    "    output_core_dims = [[]],\n",
    "    vectorize=True,\n",
    "    kwargs=dict(y=grisdale_latent.data)\n",
    ")\n",
    "\n",
    "grisdale_similarity.plot.imshow()\n",
    "plt.scatter(grisdale[\"x\"], grisdale[\"y\"], marker=\"*\", c=\"purple\", edgecolor=\"black\", s=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is admittedly very similar to if we had just selected elevation bands :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Our goal with this notebook has been to show how xbatcher supports linking `xarray` objects with deep learning models, and with converting model output back into labeled `xarray` objects. We have demonstrated two examples of reconstructing model output, both when tensor shape changes and when it does not.\n",
    "\n",
    "If you encounter any issues, please open an issue on the GitHub repository for this cookbook. Other feedback is welcome!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookbook-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ]
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
