{"version":2,"kind":"Notebook","sha256":"1aeea597b1aa61e6b1659b7a0708f311ae46cac815339c297b7a87d388ac5291","slug":"notebooks.inference-testing","location":"/notebooks/inference-testing.ipynb","dependencies":[],"frontmatter":{"title":"Infer model on array","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"cookbook-dev","language":"python"},"authors":[{"nameParsed":{"literal":"Keenan Ganz, Nabin Kalauni, and The Project Pythia Community","given":"and The Project Pythia Community","family":"Keenan Ganz","suffix":"Nabin Kalauni"},"name":"Keenan Ganz, Nabin Kalauni, and The Project Pythia Community","id":"contributors-myst-generated-uid-0"}],"open_access":true,"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"Apache-2.0","url":"https://opensource.org/licenses/Apache-2.0","name":"Apache License 2.0","free":true,"osi":true}},"github":"https://github.com/projectpythia/xbatcher-deep-learning","copyright":"2025","affiliations":[{"id":"UAlbany","name":"University at Albany (SUNY)","department":"Atmospheric and Environmental Sciences","url":"https://www.albany.edu/daes"},{"id":"CISL","name":"NSF National Center for Atmospheric Research","department":"Computational and Information Systems Lab","url":"https://www.cisl.ucar.edu"},{"id":"Unidata","name":"NSF Unidata","url":"https://www.unidata.ucar.edu"},{"id":"Argonne","name":"Argonne National Laboratory","department":"Environmental Science Division","url":"https://www.anl.gov/evs"},{"id":"CarbonPlan","name":"CarbonPlan","url":"https://carbonplan.org"},{"id":"NVIDIA","name":"NVIDIA Corporation","url":"https://www.nvidia.com/"}],"numbering":{"title":{"offset":1}},"edit_url":"https://github.com/projectpythia/xbatcher-deep-learning/blob/main/notebooks/inference-testing.ipynb","exports":[{"format":"ipynb","filename":"inference-testing.ipynb","url":"/xbatcher-deep-learning/build/inference-testing-291713f84d3640aa8ce041134548ff46.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"thematicBreak","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XtT3Ux4IQO"}],"key":"LVKX8fjCBQ"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Imports","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AeoFY1M4tJ"}],"identifier":"imports","label":"Imports","html_id":"imports","implicit":true,"key":"a8mxP7HHQZ"}],"key":"XaTEVY88Vq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import torch\nimport xbatcher\nimport xarray as xr\nimport numpy as np\nimport pytest\n\nfrom functions import _get_output_array_size, predict_on_array","key":"E3k3OQt5JH"},{"type":"output","id":"w4vmul7wURA2xquky0O0f","data":[{"output_type":"error","traceback":"\u001b[31m---------------------------------------------------------------------------\u001b[39m\n\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)\n\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxr\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytest\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _get_output_array_size, predict_on_array\n\n\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pytest'","ename":"ModuleNotFoundError","evalue":"No module named 'pytest'"}],"key":"FfKjmsLnJ7"}],"key":"ED0kYknyKC"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Testing the array size function","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P1loKsk8q7"}],"identifier":"testing-the-array-size-function","label":"Testing the array size function","html_id":"testing-the-array-size-function","implicit":true,"key":"cfriA4cezl"}],"key":"p5jg3ZTGdN"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%writefile test_get_array_size.py\nimport torch\nimport xbatcher\nimport xarray as xr\nimport numpy as np\nimport pytest\n\nfrom functions import _get_output_array_size, _get_resample_factor","key":"aZ6xsGQsjj"},{"type":"output","id":"EyPuGzQLMznWrATOJCu6r","data":[],"key":"tx41S1vGD0"}],"key":"EcmrYaq8tc"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%writefile -a test_get_array_size.py\n\n@pytest.fixture\ndef bgen_fixture() -> xbatcher.BatchGenerator:\n    data = xr.DataArray(\n        data=np.random.rand(100, 100, 10),\n        dims=(\"x\", \"y\", \"t\"),\n        coords={\n            \"x\": np.arange(100),\n            \"y\": np.arange(100),\n            \"t\": np.arange(10),\n        }\n    )\n    \n    bgen = xbatcher.BatchGenerator(\n        data,\n        input_dims=dict(x=10, y=10),\n        input_overlap=dict(x=5, y=5),\n    )\n    return bgen\n\n@pytest.mark.parametrize(\n    \"case_description, output_tensor_dim, new_dim, core_dim, resample_dim, expected_output\",\n    [\n        (\n            \"Resampling only: Downsample x, Upsample y\",\n            {'x': 5, 'y': 20},  \n            [],\n            [],\n            ['x', 'y'],\n            {'x': 50, 'y': 200} \n        ),\n        (\n            \"New dimensions only: Add a 'channel' dimension\",\n            {'channel': 3},\n            ['channel'],\n            [],\n            [],\n            {'channel': 3}\n        ),\n        (\n            \"Mixed: Resample x, add new channel dimension and keep t as core\",\n            {'x': 30, 'channel': 12, 't': 10}, \n            ['channel'],\n            ['t'],\n            ['x'],\n            {'x': 300, 'channel': 12, 't': 10} \n        ),\n        (\n            \"Identity resampling (ratio=1)\",\n            {'x': 10, 'y': 10},\n            [],\n            [],\n            ['x', 'y'],\n            {'x': 100, 'y': 100} \n        ),\n        (\n            \"Core dims only: 't' is a core dim\",\n            {'t': 10},\n            [], \n            ['t'], \n            [],\n            {'t': 10}\n        ),\n    ]\n)\ndef test_get_output_array_size_scenarios(\n    bgen_fixture,  # The fixture is passed as an argument\n    case_description,\n    output_tensor_dim,\n    new_dim,\n    core_dim,\n    resample_dim,\n    expected_output\n):\n    \"\"\"\n    Tests various valid scenarios for calculating the output array size.\n    The `case_description` parameter is not used in the code but helps make\n    test results more readable.\n    \"\"\"\n    # The `bgen_fixture` argument is the BatchGenerator instance created by our fixture\n    result = _get_output_array_size(\n        bgen=bgen_fixture,\n        output_tensor_dim=output_tensor_dim,\n        new_dim=new_dim,\n        core_dim=core_dim,\n        resample_dim=resample_dim\n    )\n    \n    assert result == expected_output, f\"Failed on case: {case_description}\"","key":"HmcQ9nItni"},{"type":"output","id":"OrhUTNt3GUdxJm0jwe9U9","data":[],"key":"boksB0dkff"}],"key":"zi53vPSZ3K"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%writefile -a test_get_array_size.py\n\ndef test_get_output_array_size_raises_error_on_mismatched_core_dim(bgen_fixture):\n    \"\"\"Tests ValueError when a core_dim size doesn't match the source.\"\"\"\n    with pytest.raises(ValueError, match=\"does not equal the source data array size\"):\n        _get_output_array_size(\n            bgen_fixture, output_tensor_dim={'t': 99}, new_dim=[], core_dim=['t'], resample_dim=[]\n        )\n\ndef test_get_output_array_size_raises_error_on_unspecified_dim(bgen_fixture):\n    \"\"\"Tests ValueError when a dimension is not specified in any category.\"\"\"\n    with pytest.raises(ValueError, match=\"must be specified in one of\"):\n        _get_output_array_size(\n            bgen_fixture, output_tensor_dim={'x': 10}, new_dim=[], core_dim=[], resample_dim=[]\n        )\n\ndef test_get_resample_factor_raises_error_on_invalid_ratio(bgen_fixture):\n    \"\"\"Tests AssertionError when the resample ratio is not an integer or its inverse.\"\"\"\n    with pytest.raises(AssertionError, match=\"must be an integer or its inverse\"):\n        # 15 / 10 = 1.5, which is not a valid ratio\n        _get_resample_factor(bgen_fixture, output_tensor_dim={'x': 15}, resample_dim=['x'])","key":"tWHR7pt0pQ"},{"type":"output","id":"qVuwoAdAcpx6MmMeveVhL","data":[],"key":"tPyqYOAnjl"}],"key":"RRCo2VT5ut"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"!pytest -v test_get_array_size.py","key":"wLnfX8DaM5"},{"type":"output","id":"AqL-HBTrKxJJQafLj9xYN","data":[],"key":"iXjKgpxPmA"}],"key":"bL2RKfbQ9R"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Testing the predict_on_array function","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"h9jk3Aq0aV"}],"identifier":"testing-the-predict-on-array-function","label":"Testing the predict_on_array function","html_id":"testing-the-predict-on-array-function","implicit":true,"key":"hJfwmeIxtp"}],"key":"qM9WrrnzR3"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%writefile test_predict_on_array.py\nimport xarray as xr\nimport numpy as np\nimport torch\nimport xbatcher\nimport pytest\nfrom xbatcher.loaders.torch import MapDataset\n\nfrom functions import _get_output_array_size, _resample_coordinate\nfrom functions import predict_on_array, _get_resample_factor\nfrom dummy_models import Identity, MeanAlongDim, SubsetAlongAxis, ExpandAlongAxis, AddAxis","key":"fJ9cLkCVx8"},{"type":"output","id":"rlqQ6qnRhTJmdU1ngInWu","data":[],"key":"qAFleHvXft"}],"key":"b1CPD5IGUO"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import xarray as xr\nimport numpy as np\nimport torch\nimport xbatcher\nimport pytest\nfrom xbatcher.loaders.torch import MapDataset\n\nfrom functions import *\nfrom dummy_models import *","key":"ByoOo3n3kD"},{"type":"output","id":"MVGrRR537abxIPuXRl86w","data":[],"key":"KzdCbhMNbq"}],"key":"MDXZWhVZFY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"input_tensor = torch.arange(125).reshape((5, 5, 5)).to(torch.float32)\ninput_tensor[0,0,:]","key":"SdNWw4vZyr"},{"type":"output","id":"KWCFNb7quOwjJ2ELc1awd","data":[],"key":"ob6SAzpKTS"}],"key":"VGFxOMgXle"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model = ExpandAlongAxis(1, 2)\nmodel(input_tensor).shape","key":"tWRyY1Nj9V"},{"type":"output","id":"GxHuMjiYRT_NVZ2JpRSOV","data":[],"key":"gPpOw0aCy1"}],"key":"J5buyEMda8"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%writefile -a test_predict_on_array.py\n\n@pytest.fixture\ndef map_dataset_fixture() -> MapDataset:\n    data = xr.DataArray(\n        data=np.arange(20 * 10).reshape(20, 10).astype(np.float32),\n        dims=(\"x\", \"y\"),\n        coords={\"x\": np.arange(20, dtype=float), \"y\": np.arange(10, dtype=float)},\n    )\n    bgen = xbatcher.BatchGenerator(data, input_dims=dict(x=10, y=5), input_overlap=dict(x=2, y=2))\n    return MapDataset(bgen)\n","key":"yejJ5apIrV"},{"type":"output","id":"92GkwOP7hPs9qNBkRuEIw","data":[],"key":"Bctn7wb3qK"}],"key":"GjMy2MdKhl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"data = xr.DataArray(\n    data=np.arange(20 * 10).reshape(20, 10),\n    dims=(\"x\", \"y\"),\n    coords={\"x\": np.arange(20), \"y\": np.arange(10)}\n).astype(float)\n\nbgen = xbatcher.BatchGenerator(\n    data,\n    input_dims=dict(x=10, y=5),\n    input_overlap=dict(x=2, y=2)\n)","key":"XQ8PiMs6v8"},{"type":"output","id":"cwQH9A2-YV2wbwccTt1eL","data":[],"key":"mSvf7LQdLv"}],"key":"hdr6AGZLfK"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ds = MapDataset(bgen)","key":"iyncdn03u3"},{"type":"output","id":"gIxHAv85rtsrgr8Ne_nBu","data":[],"key":"QNVMd0umxg"}],"key":"V7PUeR00fX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"data","key":"UhKswCvtLp"},{"type":"output","id":"zXGY9wUHm1L-Qv-mzy3CB","data":[],"key":"mGWWiE2Jzd"}],"key":"vxE6666dF8"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ds[1]","key":"Q2OzJ5SarD"},{"type":"output","id":"jXZghCfrCG-catUypx8nw","data":[],"key":"RCuGvseZOR"}],"key":"TXbiTLN9N9"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"output_tensor_dim = {'x': 20, 'y': 5}\nresample_dim = ['x', 'y']\ncore_dim = []\nnew_dim = []","key":"wwDN8sNxI7"},{"type":"output","id":"4k2n-qG_I9M5gxNhpHTXe","data":[],"key":"KKZvlOubaw"}],"key":"ktHRTE7V3l"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ds[0].shape","key":"zlys6e4MZB"},{"type":"output","id":"ccBU3nnwpbhMVsAgqdB21","data":[],"key":"aUx2D8raMu"}],"key":"Shg5FH5iPt"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model(ds[0]).shape","key":"igT0FUcrvv"},{"type":"output","id":"sZDsFLXXTQ-6FU4YRSNYD","data":[],"key":"kGQVyPUdGa"}],"key":"mFaiCiuVFm"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import functions\nfrom importlib import reload\nreload(functions)\nresult = functions.predict_on_array(\n    ds,\n    model,\n    output_tensor_dim=output_tensor_dim,\n    new_dim=new_dim,\n    core_dim=core_dim,\n    resample_dim=resample_dim,\n    batch_size=4\n)","key":"Doz4r1XBpt"},{"type":"output","id":"B7u3opaDjnWci8tRA-ypu","data":[],"key":"FeYGgYiFcN"}],"key":"QnHPlr9u6J"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%writefile -a test_predict_on_array.py\n\n@pytest.mark.parametrize(\"factor, mode, expected\", [\n    (2.0, \"edges\", np.arange(0, 10, 0.5)),\n    (0.5, \"edges\", np.arange(0, 10, 2.0)),\n])\ndef test_resample_coordinate(factor, mode, expected):\n    coord = xr.DataArray(np.arange(10, dtype=float), dims=\"x\")\n    resampled = _resample_coordinate(coord, factor, mode)\n    np.testing.assert_allclose(resampled, expected)","key":"LJg5KN6wmI"},{"type":"output","id":"3XV5FNXTxgx8NcpTgdEeB","data":[],"key":"jcy47m1ZLf"}],"key":"nqh9gMayd4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%writefile -a test_predict_on_array.py\n\n@pytest.mark.parametrize(\n    \"model, output_tensor_dim, new_dim, core_dim, resample_dim, manual_transform\",\n    [\n        # Case 1: Identity - No change\n        (\n            Identity(),\n            {'x': 10, 'y': 5},\n            [], [], ['x', 'y'],\n            lambda da: da.data\n        ),\n        # Case 2: ExpandAlongAxis - Upsampling\n        (\n            ExpandAlongAxis(ax=1, n_repeats=2), # ax=1 is 'x'\n            {'x': 20, 'y': 5},\n            [], [], ['x', 'y'],\n            lambda da: da.data.repeat(2, axis=0) # axis=0 in the 2D numpy array\n        ),\n        # Case 3: SubsetAlongAxis - Coarsening\n        (\n            SubsetAlongAxis(ax=1, n=5), # ax=1 is 'x'\n            {'x': 5, 'y': 5},\n            [], [], ['x', 'y'],\n            lambda da: da.isel(x=slice(0, 5)).data\n        ),\n        # Case 4: MeanAlongDim - Dimension reduction\n        (\n            MeanAlongDim(ax=2), # ax=2 is 'y'\n            {'x': 10},\n            [], [], ['x'],\n            lambda da: da.mean(dim='y').data\n        ),\n        # Case 5: AddAxis - Add a new dimension\n        (\n            AddAxis(ax=1), # Add new dim at axis 1\n            {'channel': 1, 'x': 10, 'y': 5},\n            ['channel'], [], ['x', 'y'],\n            lambda da: np.expand_dims(da.data, axis=0)\n        ),\n    ]\n)\ndef test_predict_on_array_all_models(\n    map_dataset_fixture, model, output_tensor_dim, new_dim, core_dim, resample_dim, manual_transform\n):\n    \"\"\"\n    Tests reassembly, averaging, and coordinate assignment using a variety of models.\n    \"\"\"\n    dataset = map_dataset_fixture\n    bgen = dataset.X_generator\n    resample_factor = _get_resample_factor(bgen, output_tensor_dim, resample_dim)\n\n    # --- Run the function under test ---\n    result_da = predict_on_array(\n        dataset=dataset, model=model, output_tensor_dim=output_tensor_dim,\n        new_dim=new_dim, core_dim=core_dim, resample_dim=resample_dim, batch_size=4\n    )\n\n    # --- Manually calculate the expected result ---\n    expected_size = _get_output_array_size(bgen, output_tensor_dim, new_dim, core_dim, resample_dim)\n    expected_sum = xr.DataArray(np.zeros(list(expected_size.values())), dims=list(expected_size.keys()))\n    expected_count = xr.full_like(expected_sum, 0, dtype=int)\n\n    for i in range(len(dataset)):\n        batch_da = bgen[i]\n        old_indexer = bgen._batch_selectors.selectors[i][0]\n        new_indexer = {}\n        for key in old_indexer:\n            if key in resample_dim:\n                new_indexer[key] = slice(int(old_indexer[key].start * resample_factor.get(key, 1)), int(old_indexer[key].stop * resample_factor.get(key, 1)))\n            elif key in core_dim:\n                new_indexer[key] = old_indexer[key]\n\n        model_output_on_batch = manual_transform(batch_da)\n        print(f\"Batch {i}: {new_indexer} -> {model_output_on_batch.shape}\")\n        print(f\"Expected sum shape: {expected_sum.loc[new_indexer].shape}\")\n        expected_sum.loc[new_indexer] += model_output_on_batch\n        expected_count.loc[new_indexer] += 1\n        \n    expected_avg_data = expected_sum.data / expected_count.data\n    \n    # --- Assert correctness ---\n    np.testing.assert_allclose(result_da.values, expected_avg_data, equal_nan=True)","key":"Fh8BzBj3Ac"},{"type":"output","id":"OPFji5AaVbDLKmg-MEnC8","data":[],"key":"yOVYSJm7Fi"}],"key":"s6QylhC6vh"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"!pytest -v test_predict_on_array.py","key":"G1XO34JgtJ"},{"type":"output","id":"tdr_FvXK_ln4awGc2l_ld","data":[],"key":"dbMF1G9Ge2"}],"key":"AkCGTLDRtz"}],"key":"lrVEwjjLeq"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"How to Cite This Cookbook","url":"/notebooks/how-to-cite","group":"Preamble"},"next":{"title":"Using xbatcher to train an autoencoder","url":"/notebooks/autoencoder","group":"Example with topography data"}}},"domain":"http://localhost:3000"}