{"version":"1","records":[{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook"},"content":"\n\n\n\n\n\n\n\n\n\nThis Project Pythia Cookbook covers a workflow for using Xarray and xbatcher for deep learning applications. Specifically, it demonstrates a reusable workflow for recreating an xarray dataset from a deep learning model’s output, which can be used for further analysis or visualization.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl2":"Motivation"},"content":"This cookbook will be useful for data scientists and machine learning practitioners who want to leverage the power of xarray and xbatcher for their deep learning workflows. By the end of this cookbook, you will have gained skills in loading and processing Xarray datasets into a format suitable for deep learning using xbatcher and furthermore, you will learn how to recreate an Xarray dataset from the output of a deep learning model.","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl2":"Authors"},"content":"Keenan Ganz, \n\nNabin Kalauni","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl2":"Structure"},"content":"This cookbook is broken up into two main sections - “xbatcher Fundamentals” and “Example Workflow”. The first section covers the foundational concepts and tools needed to work with xbatcher and xarray, while the second section provides a practical example of how to use these tools in a complete end-to-end workflow.","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl3":"xbatcher Fundamentals","lvl2":"Structure"},"type":"lvl3","url":"/#xbatcher-fundamentals","position":10},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl3":"xbatcher Fundamentals","lvl2":"Structure"},"content":"The foundational content includes an overview of xbatcher, its key features, and how it integrates with xarray for efficient data handling in deep learning workflows. The first chapter covers using xbatcher to create batches of data from an xarray dataset whereas the second chapter focuses on recreating an xarray dataset from the output of a deep learning model.","type":"content","url":"/#xbatcher-fundamentals","position":11},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl3":"Example Workflow","lvl2":"Structure"},"type":"lvl3","url":"/#example-workflow","position":12},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl3":"Example Workflow","lvl2":"Structure"},"content":"Example workflow includes using xbatcher to create batches of data from an xarray dataset (ASTER Global Digital Elevation model), training an Autoencoder on this data, and then using xbatcher again to reassemble the model’s output into a new xarray dataset.","type":"content","url":"/#example-workflow","position":13},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":14},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":15},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":16},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.\n\nNote, not all Cookbook chapters are executable. If you do not see\nthe rocket ship icon, such as on this page, you are not viewing an\nexecutable book chapter.","type":"content","url":"/#running-on-binder","position":17},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":18},{"hierarchy":{"lvl1":"Xarray for Deep Learning Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\nClone the https://github.com/ProjectPythia/xbatcher-deep-learning repository: git clone https://github.com/ProjectPythia/xbatcher-deep-learning.git\n\nMove into the xbatcher-deep-learning directorycd xbatcher-deep-learning\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate cookbook-dev\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":19},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder"},"type":"lvl1","url":"/notebooks/autoencoder","position":0},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder"},"content":"\n\n","type":"content","url":"/notebooks/autoencoder","position":1},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/autoencoder#imports","position":2},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Imports"},"content":"\n\nimport os\nfrom importlib import reload\n\n# DL stuff\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data\n\ntorch.set_default_dtype(torch.float64)\n\n# Geospatial stuff\nimport xarray as xr\nimport xbatcher\nimport rioxarray\nimport xbatcher\nfrom xbatcher.loaders.torch import MapDataset\n\n# Etc\nimport numpy as np\nfrom numpy.linalg import norm\nfrom matplotlib import pyplot as plt\nfrom tqdm.autonotebook import tqdm\n\n# Locals\nimport functions\nimport autoencoder\n\n","type":"content","url":"/notebooks/autoencoder#imports","position":3},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Get data"},"type":"lvl2","url":"/notebooks/autoencoder#get-data","position":4},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Get data"},"content":"\n\nWe will start by pulling a segment of NASADEM for Washington’s Olympic peninsula.\n\n# Rasterio adds a blank edge. Trim these out.\ndem = rioxarray.open_rasterio(\"../ASTGTMV003_N47W124_dem.tif\")\ndem = dem.isel(y=slice(0, -1), x=slice(0, -1))\ndem = (dem - dem.min()) / (dem.max() - dem.min())\ndem\n\ndem.isel(band=0).plot.imshow(cmap=\"terrain\")\n\n","type":"content","url":"/notebooks/autoencoder#get-data","position":5},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Generate training examples"},"type":"lvl2","url":"/notebooks/autoencoder#generate-training-examples","position":6},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Generate training examples"},"content":"Here, we use xbatcher to window patches of terrain in the same location.\n\nbgen_x = xbatcher.BatchGenerator(\n    dem,\n    input_dims=dict(x=32, y=32),\n    input_overlap=dict(x=16, y=16)\n)\n\nds = MapDataset(\n    X_generator=bgen_x,)\n\nloader = torch.utils.data.DataLoader(ds, batch_size=16, shuffle=True)\n\nX = next(iter(loader))\n\nprint(\"Input tensor shape:\", X.shape)\n\n","type":"content","url":"/notebooks/autoencoder#generate-training-examples","position":7},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Model setup"},"type":"lvl2","url":"/notebooks/autoencoder#model-setup","position":8},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Model setup"},"content":"\n\nm = autoencoder.Autoencoder(base_channel_size=32, latent_dim=64, num_input_channels=1, width=32, height=32)\nopt = m._configure_optimizers()\n\nout = m(X)\nprint(out.shape)\n\n","type":"content","url":"/notebooks/autoencoder#model-setup","position":9},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Model training"},"type":"lvl2","url":"/notebooks/autoencoder#model-training","position":10},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Model training"},"content":"We aren’t using pytorch-lightning and load a pre-trained model here to keep the notebook environment lean. For your project, we highly recommend using a framework to abstract away much of the boilerplate code below.\n\ndef train_one_epoch(epoch_index):\n    last_loss = 0.\n    running_loss = 0.\n    \n    for i, batch in enumerate(tqdm(loader)):\n        # Zero your gradients for every batch!\n        opt.zero_grad()\n\n        # Make predictions for this batch\n        outputs = m(batch)\n\n        # Compute the loss and its gradients\n        loss = m._get_reconstruction_loss(batch, outputs)\n        loss.backward()\n        running_loss += loss.item()\n\n        # Adjust learning weights\n        opt.step()\n\n    return running_loss / len(loader)\n\nn_epochs = 5\nfor i_epoch in range(n_epochs):\n    loss = train_one_epoch(i_epoch)\n    print(f\"Epoch {i_epoch+1:>3}: {loss:.3e}\")\n\ntorch.save(m.state_dict(), \"../autoencoder.torch\")\n\nDanger\n\nThis model is certainly overfitted. For brevity we have omitted a validation dataset, which is essential for building models that generalize well on unseen data.\n\nm.load_state_dict(torch.load(\"../autoencoder.torch\", weights_only=True))\n\nm.eval()\nn_examples = 4\ninputs = next(iter(loader))\noutputs = m(inputs)\n\ninputs = inputs.detach().cpu().numpy()\noutputs = outputs.detach().cpu().numpy()\n\nfig, axes = plt.subplots(n_examples, 2)\n\nfor i_row in range(n_examples):\n    axes[i_row, 0].imshow(inputs[i_row, 0, ...])\n    axes[i_row, 1].imshow(outputs[i_row, 0, ...])\n\nfor a in axes.flat:\n    a.set_xticks([])\n    a.set_yticks([])\n\naxes[0, 0].set_title(\"Original patch\")\naxes[0, 1].set_title(\"Reconstruction\")\n\nfig.tight_layout()\nplt.show()\n\n","type":"content","url":"/notebooks/autoencoder#model-training","position":11},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Reconstruction 1: Getting the full array back"},"type":"lvl2","url":"/notebooks/autoencoder#reconstruction-1-getting-the-full-array-back","position":12},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Reconstruction 1: Getting the full array back"},"content":"Suppose we would like to evaluate how the autoencoder does on reconstructing the entire terrain patch by combining outputs across all input patches. To do so we can use the predict_on_array function described in the previous notebook. Our model outputs tensors with shape (band=1, x=32, y=32). We need to specify each of these axes in the call to predict_on_array. channel does not change size and is not used by the BatchGenerator, so it goes in core_dim. Both x and y are used by the BatchGenerator, so although they do not change size they still go in resample_dim. That accounts for all tensor axes, so we can leave the new_dim argument as an empty list.\n\ndem_reconst = functions.predict_on_array(\n    dataset=ds,\n    model=m,\n    output_tensor_dim=dict(band=1, y=32, x=32),\n    new_dim=[],\n    core_dim=[\"band\"],\n    resample_dim=[\"x\", \"y\"]\n)\n\ndem_reconst.isel(band=0).plot.imshow(cmap=\"terrain\")\n\nThat certainly looks like the original DEM. Let’s try plotting the error in the reconstruction.\n\nerr = (dem_reconst - dem)\nerr.isel(band=0).plot.imshow()\nplt.show()\n\nerr.plot.hist()\nplt.show()\n\nNot bad!\n\n","type":"content","url":"/notebooks/autoencoder#reconstruction-1-getting-the-full-array-back","position":13},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Reconstruction 2: Getting the latent dimension"},"type":"lvl2","url":"/notebooks/autoencoder#reconstruction-2-getting-the-latent-dimension","position":14},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Reconstruction 2: Getting the latent dimension"},"content":"A common application of autoencoders is to use the latent dimension for some application. Let’s turn our autoencoder’s predictions into a data cube. To do so we will modify the batch generator to not have overlapping windows. We also have to slightly clip the size of the input DEM. This is because we are effectively downscaling the spatial axes by a factor of 32. Since 3600 / 32 is not an integer, predict_on_array will not know how to rescale the array size. So, we have to clip the DEM to the nearest integer multiple of 32. In this case the nearest multiple is 3584, which we achieve by clipping 8 pixels from each side.\n\nbgen_no_overlap = xbatcher.BatchGenerator(\n    dem.isel(x=slice(8, -8), y=slice(8, -8)),\n    input_dims=dict(x=32, y=32),\n    input_overlap=dict(x=0, y=0)\n)\n\nds_no_overlap = MapDataset(\n    X_generator=bgen_no_overlap\n)\n\nloader = torch.utils.data.DataLoader(ds_no_overlap, batch_size=16, shuffle=True)\n\nex_input = next(iter(loader))\n\n# Same as before\nprint(\"Input shape:\", ex_input.shape)\n\nNext we will write a function that the calls the encoder arm of the autoencoder and adds a fake x and y dimension.\n\ndef infer_with_encoder(x):\n    return m.encoder(x)[:, None, None, :]\n\nex_output = infer_with_encoder(ex_input)\nprint(\"Output shape:\", ex_output.shape)\n\nNow we combine the outputs together into a new data cube.\n\nlatent_dim_cube = functions.predict_on_array(\n    dataset=ds_no_overlap,\n    model=infer_with_encoder,\n    output_tensor_dim=dict(y=1, x=1, channel=64),\n    new_dim=[\"channel\"],\n    core_dim=[],\n    resample_dim=[\"x\", \"y\"]\n)\n\nlatent_dim_cube\n\nNote that despite substantially re-arranging the input DataArray, we have retained the coordinate information at a resampled resolution.\n\nIf we simply sum the output over the channel dimension, we see that the encoder clearly distinguishes between upland and lowland areas.\n\nlatent_dim_cube.sum(dim=\"channel\").plot.imshow()\n\nAs a final demonstration of this workflow, let’s compute the cosine similarity of each of the below pixels with the latent encoding of \n\nMt. Olympus.\n\nolympus = dict(x=-123.7066, y=47.7998)\nolympus_latent = latent_dim_cube.sel(**olympus, method=\"nearest\")\nolympus_latent\n\ndef numpy_cosine_similarity(x, y):\n    return np.dot(x, y)/(norm(x)*norm(y))\n\nolympus_similarity = xr.apply_ufunc(\n    numpy_cosine_similarity,\n    latent_dim_cube,\n    input_core_dims = [[\"channel\"]],\n    output_core_dims = [[]],\n    vectorize=True,\n    kwargs=dict(y=olympus_latent.data)\n)\n\nolympus_similarity.plot.imshow()\nplt.scatter(olympus[\"x\"], olympus[\"y\"], marker=\"*\", c=\"purple\", edgecolor=\"black\", s=200)\nplt.title(\"Cosine similarity with Mt. Olympus, WA\")\nplt.show()\n\nSimilarly, we can identify foothills with similar topography to Grisdale, WA.\n\ngrisdale = dict(y=47.356625678465925, x=-123.61183314426664)\ngrisdale_latent = latent_dim_cube.sel(**grisdale, method=\"nearest\")\n\ngrisdale_similarity = xr.apply_ufunc(\n    numpy_cosine_similarity,\n    latent_dim_cube,\n    input_core_dims = [[\"channel\"]],\n    output_core_dims = [[]],\n    vectorize=True,\n    kwargs=dict(y=grisdale_latent.data)\n)\n\ngrisdale_similarity.plot.imshow()\nplt.scatter(grisdale[\"x\"], grisdale[\"y\"], marker=\"*\", c=\"purple\", edgecolor=\"black\", s=200)\nplt.show()\n\nThis result is admittedly very similar to if we had just selected elevation bands :)\n\n\n\n","type":"content","url":"/notebooks/autoencoder#reconstruction-2-getting-the-latent-dimension","position":15},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/autoencoder#summary","position":16},{"hierarchy":{"lvl1":"Using xbatcher to train an autoencoder","lvl2":"Summary"},"content":"Our goal with this notebook has been to show how xbatcher supports linking xarray objects with deep learning models, and with converting model output back into labeled xarray objects. We have demonstrated two examples of reconstructing model output, both when tensor shape changes and when it does not.\n\nIf you encounter any issues, please open an issue on the GitHub repository for this cookbook. Other feedback is welcome!","type":"content","url":"/notebooks/autoencoder#summary","position":17},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1},{"hierarchy":{"lvl1":"Infer model on array"},"type":"lvl1","url":"/notebooks/inference-testing","position":0},{"hierarchy":{"lvl1":"Infer model on array"},"content":"\n\n","type":"content","url":"/notebooks/inference-testing","position":1},{"hierarchy":{"lvl1":"Infer model on array","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/inference-testing#imports","position":2},{"hierarchy":{"lvl1":"Infer model on array","lvl2":"Imports"},"content":"\n\nimport torch\nimport xbatcher\nimport xarray as xr\nimport numpy as np\nimport pytest\n\nfrom functions import _get_output_array_size, predict_on_array\n\n","type":"content","url":"/notebooks/inference-testing#imports","position":3},{"hierarchy":{"lvl1":"Infer model on array","lvl2":"Testing the array size function"},"type":"lvl2","url":"/notebooks/inference-testing#testing-the-array-size-function","position":4},{"hierarchy":{"lvl1":"Infer model on array","lvl2":"Testing the array size function"},"content":"\n\n%%writefile test_get_array_size.py\nimport torch\nimport xbatcher\nimport xarray as xr\nimport numpy as np\nimport pytest\n\nfrom functions import _get_output_array_size, _get_resample_factor\n\n%%writefile -a test_get_array_size.py\n\n@pytest.fixture\ndef bgen_fixture() -> xbatcher.BatchGenerator:\n    data = xr.DataArray(\n        data=np.random.rand(100, 100, 10),\n        dims=(\"x\", \"y\", \"t\"),\n        coords={\n            \"x\": np.arange(100),\n            \"y\": np.arange(100),\n            \"t\": np.arange(10),\n        }\n    )\n    \n    bgen = xbatcher.BatchGenerator(\n        data,\n        input_dims=dict(x=10, y=10),\n        input_overlap=dict(x=5, y=5),\n    )\n    return bgen\n\n@pytest.mark.parametrize(\n    \"case_description, output_tensor_dim, new_dim, core_dim, resample_dim, expected_output\",\n    [\n        (\n            \"Resampling only: Downsample x, Upsample y\",\n            {'x': 5, 'y': 20},  \n            [],\n            [],\n            ['x', 'y'],\n            {'x': 50, 'y': 200} \n        ),\n        (\n            \"New dimensions only: Add a 'channel' dimension\",\n            {'channel': 3},\n            ['channel'],\n            [],\n            [],\n            {'channel': 3}\n        ),\n        (\n            \"Mixed: Resample x, add new channel dimension and keep t as core\",\n            {'x': 30, 'channel': 12, 't': 10}, \n            ['channel'],\n            ['t'],\n            ['x'],\n            {'x': 300, 'channel': 12, 't': 10} \n        ),\n        (\n            \"Identity resampling (ratio=1)\",\n            {'x': 10, 'y': 10},\n            [],\n            [],\n            ['x', 'y'],\n            {'x': 100, 'y': 100} \n        ),\n        (\n            \"Core dims only: 't' is a core dim\",\n            {'t': 10},\n            [], \n            ['t'], \n            [],\n            {'t': 10}\n        ),\n    ]\n)\ndef test_get_output_array_size_scenarios(\n    bgen_fixture,  # The fixture is passed as an argument\n    case_description,\n    output_tensor_dim,\n    new_dim,\n    core_dim,\n    resample_dim,\n    expected_output\n):\n    \"\"\"\n    Tests various valid scenarios for calculating the output array size.\n    The `case_description` parameter is not used in the code but helps make\n    test results more readable.\n    \"\"\"\n    # The `bgen_fixture` argument is the BatchGenerator instance created by our fixture\n    result = _get_output_array_size(\n        bgen=bgen_fixture,\n        output_tensor_dim=output_tensor_dim,\n        new_dim=new_dim,\n        core_dim=core_dim,\n        resample_dim=resample_dim\n    )\n    \n    assert result == expected_output, f\"Failed on case: {case_description}\"\n\n%%writefile -a test_get_array_size.py\n\ndef test_get_output_array_size_raises_error_on_mismatched_core_dim(bgen_fixture):\n    \"\"\"Tests ValueError when a core_dim size doesn't match the source.\"\"\"\n    with pytest.raises(ValueError, match=\"does not equal the source data array size\"):\n        _get_output_array_size(\n            bgen_fixture, output_tensor_dim={'t': 99}, new_dim=[], core_dim=['t'], resample_dim=[]\n        )\n\ndef test_get_output_array_size_raises_error_on_unspecified_dim(bgen_fixture):\n    \"\"\"Tests ValueError when a dimension is not specified in any category.\"\"\"\n    with pytest.raises(ValueError, match=\"must be specified in one of\"):\n        _get_output_array_size(\n            bgen_fixture, output_tensor_dim={'x': 10}, new_dim=[], core_dim=[], resample_dim=[]\n        )\n\ndef test_get_resample_factor_raises_error_on_invalid_ratio(bgen_fixture):\n    \"\"\"Tests AssertionError when the resample ratio is not an integer or its inverse.\"\"\"\n    with pytest.raises(AssertionError, match=\"must be an integer or its inverse\"):\n        # 15 / 10 = 1.5, which is not a valid ratio\n        _get_resample_factor(bgen_fixture, output_tensor_dim={'x': 15}, resample_dim=['x'])\n\n!pytest -v test_get_array_size.py\n\n","type":"content","url":"/notebooks/inference-testing#testing-the-array-size-function","position":5},{"hierarchy":{"lvl1":"Infer model on array","lvl2":"Testing the predict_on_array function"},"type":"lvl2","url":"/notebooks/inference-testing#testing-the-predict-on-array-function","position":6},{"hierarchy":{"lvl1":"Infer model on array","lvl2":"Testing the predict_on_array function"},"content":"\n\n%%writefile test_predict_on_array.py\nimport xarray as xr\nimport numpy as np\nimport torch\nimport xbatcher\nimport pytest\nfrom xbatcher.loaders.torch import MapDataset\n\nfrom functions import _get_output_array_size, _resample_coordinate\nfrom functions import predict_on_array, _get_resample_factor\nfrom dummy_models import Identity, MeanAlongDim, SubsetAlongAxis, ExpandAlongAxis, AddAxis\n\nimport xarray as xr\nimport numpy as np\nimport torch\nimport xbatcher\nimport pytest\nfrom xbatcher.loaders.torch import MapDataset\n\nfrom functions import *\nfrom dummy_models import *\n\ninput_tensor = torch.arange(125).reshape((5, 5, 5)).to(torch.float32)\ninput_tensor[0,0,:]\n\nmodel = ExpandAlongAxis(1, 2)\nmodel(input_tensor).shape\n\n%%writefile -a test_predict_on_array.py\n\n@pytest.fixture\ndef map_dataset_fixture() -> MapDataset:\n    data = xr.DataArray(\n        data=np.arange(20 * 10).reshape(20, 10).astype(np.float32),\n        dims=(\"x\", \"y\"),\n        coords={\"x\": np.arange(20, dtype=float), \"y\": np.arange(10, dtype=float)},\n    )\n    bgen = xbatcher.BatchGenerator(data, input_dims=dict(x=10, y=5), input_overlap=dict(x=2, y=2))\n    return MapDataset(bgen)\n\n\ndata = xr.DataArray(\n    data=np.arange(20 * 10).reshape(20, 10),\n    dims=(\"x\", \"y\"),\n    coords={\"x\": np.arange(20), \"y\": np.arange(10)}\n).astype(float)\n\nbgen = xbatcher.BatchGenerator(\n    data,\n    input_dims=dict(x=10, y=5),\n    input_overlap=dict(x=2, y=2)\n)\n\nds = MapDataset(bgen)\n\ndata\n\nds[1]\n\noutput_tensor_dim = {'x': 20, 'y': 5}\nresample_dim = ['x', 'y']\ncore_dim = []\nnew_dim = []\n\nds[0].shape\n\nmodel(ds[0]).shape\n\nimport functions\nfrom importlib import reload\nreload(functions)\nresult = functions.predict_on_array(\n    ds,\n    model,\n    output_tensor_dim=output_tensor_dim,\n    new_dim=new_dim,\n    core_dim=core_dim,\n    resample_dim=resample_dim,\n    batch_size=4\n)\n\n%%writefile -a test_predict_on_array.py\n\n@pytest.mark.parametrize(\"factor, mode, expected\", [\n    (2.0, \"edges\", np.arange(0, 10, 0.5)),\n    (0.5, \"edges\", np.arange(0, 10, 2.0)),\n])\ndef test_resample_coordinate(factor, mode, expected):\n    coord = xr.DataArray(np.arange(10, dtype=float), dims=\"x\")\n    resampled = _resample_coordinate(coord, factor, mode)\n    np.testing.assert_allclose(resampled, expected)\n\n%%writefile -a test_predict_on_array.py\n\n@pytest.mark.parametrize(\n    \"model, output_tensor_dim, new_dim, core_dim, resample_dim, manual_transform\",\n    [\n        # Case 1: Identity - No change\n        (\n            Identity(),\n            {'x': 10, 'y': 5},\n            [], [], ['x', 'y'],\n            lambda da: da.data\n        ),\n        # Case 2: ExpandAlongAxis - Upsampling\n        (\n            ExpandAlongAxis(ax=1, n_repeats=2), # ax=1 is 'x'\n            {'x': 20, 'y': 5},\n            [], [], ['x', 'y'],\n            lambda da: da.data.repeat(2, axis=0) # axis=0 in the 2D numpy array\n        ),\n        # Case 3: SubsetAlongAxis - Coarsening\n        (\n            SubsetAlongAxis(ax=1, n=5), # ax=1 is 'x'\n            {'x': 5, 'y': 5},\n            [], [], ['x', 'y'],\n            lambda da: da.isel(x=slice(0, 5)).data\n        ),\n        # Case 4: MeanAlongDim - Dimension reduction\n        (\n            MeanAlongDim(ax=2), # ax=2 is 'y'\n            {'x': 10},\n            [], [], ['x'],\n            lambda da: da.mean(dim='y').data\n        ),\n        # Case 5: AddAxis - Add a new dimension\n        (\n            AddAxis(ax=1), # Add new dim at axis 1\n            {'channel': 1, 'x': 10, 'y': 5},\n            ['channel'], [], ['x', 'y'],\n            lambda da: np.expand_dims(da.data, axis=0)\n        ),\n    ]\n)\ndef test_predict_on_array_all_models(\n    map_dataset_fixture, model, output_tensor_dim, new_dim, core_dim, resample_dim, manual_transform\n):\n    \"\"\"\n    Tests reassembly, averaging, and coordinate assignment using a variety of models.\n    \"\"\"\n    dataset = map_dataset_fixture\n    bgen = dataset.X_generator\n    resample_factor = _get_resample_factor(bgen, output_tensor_dim, resample_dim)\n\n    # --- Run the function under test ---\n    result_da = predict_on_array(\n        dataset=dataset, model=model, output_tensor_dim=output_tensor_dim,\n        new_dim=new_dim, core_dim=core_dim, resample_dim=resample_dim, batch_size=4\n    )\n\n    # --- Manually calculate the expected result ---\n    expected_size = _get_output_array_size(bgen, output_tensor_dim, new_dim, core_dim, resample_dim)\n    expected_sum = xr.DataArray(np.zeros(list(expected_size.values())), dims=list(expected_size.keys()))\n    expected_count = xr.full_like(expected_sum, 0, dtype=int)\n\n    for i in range(len(dataset)):\n        batch_da = bgen[i]\n        old_indexer = bgen._batch_selectors.selectors[i][0]\n        new_indexer = {}\n        for key in old_indexer:\n            if key in resample_dim:\n                new_indexer[key] = slice(int(old_indexer[key].start * resample_factor.get(key, 1)), int(old_indexer[key].stop * resample_factor.get(key, 1)))\n            elif key in core_dim:\n                new_indexer[key] = old_indexer[key]\n\n        model_output_on_batch = manual_transform(batch_da)\n        print(f\"Batch {i}: {new_indexer} -> {model_output_on_batch.shape}\")\n        print(f\"Expected sum shape: {expected_sum.loc[new_indexer].shape}\")\n        expected_sum.loc[new_indexer] += model_output_on_batch\n        expected_count.loc[new_indexer] += 1\n        \n    expected_avg_data = expected_sum.data / expected_count.data\n    \n    # --- Assert correctness ---\n    np.testing.assert_allclose(result_da.values, expected_avg_data, equal_nan=True)\n\n!pytest -v test_predict_on_array.py","type":"content","url":"/notebooks/inference-testing#testing-the-predict-on-array-function","position":7}]}