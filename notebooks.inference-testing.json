{"version":2,"kind":"Notebook","sha256":"1aeea597b1aa61e6b1659b7a0708f311ae46cac815339c297b7a87d388ac5291","slug":"notebooks.inference-testing","location":"/notebooks/inference-testing.ipynb","dependencies":[],"frontmatter":{"title":"Infer model on array","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"cookbook-dev","language":"python"},"authors":[{"nameParsed":{"literal":"Keenan Ganz, Nabin Kalauni, and The Project Pythia Community","given":"and The Project Pythia Community","family":"Keenan Ganz","suffix":"Nabin Kalauni"},"name":"Keenan Ganz, Nabin Kalauni, and The Project Pythia Community","id":"contributors-myst-generated-uid-0"}],"open_access":true,"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"Apache-2.0","url":"https://opensource.org/licenses/Apache-2.0","name":"Apache License 2.0","free":true,"osi":true}},"github":"https://github.com/projectpythia/xbatcher-deep-learning","copyright":"2025","affiliations":[{"id":"UAlbany","name":"University at Albany (SUNY)","department":"Atmospheric and Environmental Sciences","url":"https://www.albany.edu/daes"},{"id":"CISL","name":"NSF National Center for Atmospheric Research","department":"Computational and Information Systems Lab","url":"https://www.cisl.ucar.edu"},{"id":"Unidata","name":"NSF Unidata","url":"https://www.unidata.ucar.edu"},{"id":"Argonne","name":"Argonne National Laboratory","department":"Environmental Science Division","url":"https://www.anl.gov/evs"},{"id":"CarbonPlan","name":"CarbonPlan","url":"https://carbonplan.org"},{"id":"NVIDIA","name":"NVIDIA Corporation","url":"https://www.nvidia.com/"}],"numbering":{"title":{"offset":1}},"edit_url":"https://github.com/projectpythia/xbatcher-deep-learning/blob/main/notebooks/inference-testing.ipynb","exports":[{"format":"ipynb","filename":"inference-testing.ipynb","url":"/xbatcher-deep-learning/build/inference-testing-9e759a039286fad71b7aa09ad84f4a16.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"thematicBreak","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CrF4uRhpXz"}],"key":"WzBO4krpUQ"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Imports","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QRWyrpFOuS"}],"identifier":"imports","label":"Imports","html_id":"imports","implicit":true,"key":"ta96aFE155"}],"key":"Q6walLyzH2"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import torch\nimport xbatcher\nimport xarray as xr\nimport numpy as np\nimport pytest\n\nfrom functions import _get_output_array_size, predict_on_array","key":"oL2HVsDLXR"},{"type":"output","id":"QC9pGzL_jpTEkeK5e77ik","data":[{"output_type":"error","traceback":"\u001b[31m---------------------------------------------------------------------------\u001b[39m\n\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)\n\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxr\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytest\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _get_output_array_size, predict_on_array\n\n\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pytest'","ename":"ModuleNotFoundError","evalue":"No module named 'pytest'"}],"key":"PiaOio7gu8"}],"key":"FjATDhvUZc"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Testing the array size function","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gcd0vref9I"}],"identifier":"testing-the-array-size-function","label":"Testing the array size function","html_id":"testing-the-array-size-function","implicit":true,"key":"m5SYtN2Q1K"}],"key":"iym6swu0g0"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%writefile test_get_array_size.py\nimport torch\nimport xbatcher\nimport xarray as xr\nimport numpy as np\nimport pytest\n\nfrom functions import _get_output_array_size, _get_resample_factor","key":"d3xt8C0iGo"},{"type":"output","id":"NJQJqBKDv5KdpBk6180dq","data":[],"key":"N12nbmcy1B"}],"key":"toSvwJqUCp"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%writefile -a test_get_array_size.py\n\n@pytest.fixture\ndef bgen_fixture() -> xbatcher.BatchGenerator:\n    data = xr.DataArray(\n        data=np.random.rand(100, 100, 10),\n        dims=(\"x\", \"y\", \"t\"),\n        coords={\n            \"x\": np.arange(100),\n            \"y\": np.arange(100),\n            \"t\": np.arange(10),\n        }\n    )\n    \n    bgen = xbatcher.BatchGenerator(\n        data,\n        input_dims=dict(x=10, y=10),\n        input_overlap=dict(x=5, y=5),\n    )\n    return bgen\n\n@pytest.mark.parametrize(\n    \"case_description, output_tensor_dim, new_dim, core_dim, resample_dim, expected_output\",\n    [\n        (\n            \"Resampling only: Downsample x, Upsample y\",\n            {'x': 5, 'y': 20},  \n            [],\n            [],\n            ['x', 'y'],\n            {'x': 50, 'y': 200} \n        ),\n        (\n            \"New dimensions only: Add a 'channel' dimension\",\n            {'channel': 3},\n            ['channel'],\n            [],\n            [],\n            {'channel': 3}\n        ),\n        (\n            \"Mixed: Resample x, add new channel dimension and keep t as core\",\n            {'x': 30, 'channel': 12, 't': 10}, \n            ['channel'],\n            ['t'],\n            ['x'],\n            {'x': 300, 'channel': 12, 't': 10} \n        ),\n        (\n            \"Identity resampling (ratio=1)\",\n            {'x': 10, 'y': 10},\n            [],\n            [],\n            ['x', 'y'],\n            {'x': 100, 'y': 100} \n        ),\n        (\n            \"Core dims only: 't' is a core dim\",\n            {'t': 10},\n            [], \n            ['t'], \n            [],\n            {'t': 10}\n        ),\n    ]\n)\ndef test_get_output_array_size_scenarios(\n    bgen_fixture,  # The fixture is passed as an argument\n    case_description,\n    output_tensor_dim,\n    new_dim,\n    core_dim,\n    resample_dim,\n    expected_output\n):\n    \"\"\"\n    Tests various valid scenarios for calculating the output array size.\n    The `case_description` parameter is not used in the code but helps make\n    test results more readable.\n    \"\"\"\n    # The `bgen_fixture` argument is the BatchGenerator instance created by our fixture\n    result = _get_output_array_size(\n        bgen=bgen_fixture,\n        output_tensor_dim=output_tensor_dim,\n        new_dim=new_dim,\n        core_dim=core_dim,\n        resample_dim=resample_dim\n    )\n    \n    assert result == expected_output, f\"Failed on case: {case_description}\"","key":"mFeKB2fUoC"},{"type":"output","id":"RVL4bhGB2l2sZ46xk5Noy","data":[],"key":"ltlhH04S2S"}],"key":"CUEahU3AwC"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%writefile -a test_get_array_size.py\n\ndef test_get_output_array_size_raises_error_on_mismatched_core_dim(bgen_fixture):\n    \"\"\"Tests ValueError when a core_dim size doesn't match the source.\"\"\"\n    with pytest.raises(ValueError, match=\"does not equal the source data array size\"):\n        _get_output_array_size(\n            bgen_fixture, output_tensor_dim={'t': 99}, new_dim=[], core_dim=['t'], resample_dim=[]\n        )\n\ndef test_get_output_array_size_raises_error_on_unspecified_dim(bgen_fixture):\n    \"\"\"Tests ValueError when a dimension is not specified in any category.\"\"\"\n    with pytest.raises(ValueError, match=\"must be specified in one of\"):\n        _get_output_array_size(\n            bgen_fixture, output_tensor_dim={'x': 10}, new_dim=[], core_dim=[], resample_dim=[]\n        )\n\ndef test_get_resample_factor_raises_error_on_invalid_ratio(bgen_fixture):\n    \"\"\"Tests AssertionError when the resample ratio is not an integer or its inverse.\"\"\"\n    with pytest.raises(AssertionError, match=\"must be an integer or its inverse\"):\n        # 15 / 10 = 1.5, which is not a valid ratio\n        _get_resample_factor(bgen_fixture, output_tensor_dim={'x': 15}, resample_dim=['x'])","key":"WxmqTqlS7v"},{"type":"output","id":"T5yFtE2DYv9lg2JtTkm5B","data":[],"key":"anM1UxhCbY"}],"key":"sjhw932uNB"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"!pytest -v test_get_array_size.py","key":"kpsblpIfmR"},{"type":"output","id":"wHUSELr_Gv065Y4mZGT07","data":[],"key":"STxF6zW9nc"}],"key":"UvKUODbqGu"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Testing the predict_on_array function","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"e4LlF9j5iX"}],"identifier":"testing-the-predict-on-array-function","label":"Testing the predict_on_array function","html_id":"testing-the-predict-on-array-function","implicit":true,"key":"mNGOn3GNTj"}],"key":"cvBumiXFSG"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%writefile test_predict_on_array.py\nimport xarray as xr\nimport numpy as np\nimport torch\nimport xbatcher\nimport pytest\nfrom xbatcher.loaders.torch import MapDataset\n\nfrom functions import _get_output_array_size, _resample_coordinate\nfrom functions import predict_on_array, _get_resample_factor\nfrom dummy_models import Identity, MeanAlongDim, SubsetAlongAxis, ExpandAlongAxis, AddAxis","key":"Wx0qsdhPQM"},{"type":"output","id":"dTwX6NafF8mgRCzDGiHW3","data":[],"key":"oGcVSFnHuH"}],"key":"IMBjLsjI65"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import xarray as xr\nimport numpy as np\nimport torch\nimport xbatcher\nimport pytest\nfrom xbatcher.loaders.torch import MapDataset\n\nfrom functions import *\nfrom dummy_models import *","key":"vn73OYKiBb"},{"type":"output","id":"jpegDbAYwgZeEeQM9wTpo","data":[],"key":"Jg9Qkhw1K8"}],"key":"sNrGrfu6BX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"input_tensor = torch.arange(125).reshape((5, 5, 5)).to(torch.float32)\ninput_tensor[0,0,:]","key":"Pn2TfRNJs9"},{"type":"output","id":"NRBoEEVQNvdbTZzTr_dC4","data":[],"key":"Vb8l08NoPc"}],"key":"PmMYKa4lq1"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model = ExpandAlongAxis(1, 2)\nmodel(input_tensor).shape","key":"AydAQuNUSG"},{"type":"output","id":"obcOUO1TdOqSrTL2HRQg5","data":[],"key":"iSVgqAezDv"}],"key":"pXwA1XOCCO"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%writefile -a test_predict_on_array.py\n\n@pytest.fixture\ndef map_dataset_fixture() -> MapDataset:\n    data = xr.DataArray(\n        data=np.arange(20 * 10).reshape(20, 10).astype(np.float32),\n        dims=(\"x\", \"y\"),\n        coords={\"x\": np.arange(20, dtype=float), \"y\": np.arange(10, dtype=float)},\n    )\n    bgen = xbatcher.BatchGenerator(data, input_dims=dict(x=10, y=5), input_overlap=dict(x=2, y=2))\n    return MapDataset(bgen)\n","key":"fqwbpuq5uY"},{"type":"output","id":"gjuhotO6Sy3mh24328WeP","data":[],"key":"GCjVP5FHLm"}],"key":"IJTknOpiut"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"data = xr.DataArray(\n    data=np.arange(20 * 10).reshape(20, 10),\n    dims=(\"x\", \"y\"),\n    coords={\"x\": np.arange(20), \"y\": np.arange(10)}\n).astype(float)\n\nbgen = xbatcher.BatchGenerator(\n    data,\n    input_dims=dict(x=10, y=5),\n    input_overlap=dict(x=2, y=2)\n)","key":"jy7RmawbIV"},{"type":"output","id":"wmD7gxbtACjeUc46oVDDw","data":[],"key":"epsY4R6SWm"}],"key":"GpMBKXKzrD"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ds = MapDataset(bgen)","key":"DkDJzMIDkD"},{"type":"output","id":"U548QccyL4gsvYRxKcvv5","data":[],"key":"EDjvcLv6Sb"}],"key":"FTCDcPy9ZQ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"data","key":"OGe3awyvdP"},{"type":"output","id":"pt43PZi9Xf-yNuAhAo6xo","data":[],"key":"ZXrGfSEKxt"}],"key":"pp219nCJpk"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ds[1]","key":"BK92rNpC0s"},{"type":"output","id":"oTza3b-98BQmBm1RfIRJr","data":[],"key":"NB5HG89Bqw"}],"key":"yO3qcHuG2e"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"output_tensor_dim = {'x': 20, 'y': 5}\nresample_dim = ['x', 'y']\ncore_dim = []\nnew_dim = []","key":"guKIIKt89v"},{"type":"output","id":"K78AFMiIvwasiDLmyf7K0","data":[],"key":"sI1kKgNSuc"}],"key":"JF3hM9cpPP"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ds[0].shape","key":"OLTZUZrKgM"},{"type":"output","id":"0p81p6uoKnVQb4qTDCxM5","data":[],"key":"fmQjcj03pj"}],"key":"ViZoYvmQyi"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model(ds[0]).shape","key":"ZeS50t84Pc"},{"type":"output","id":"DSA797Nkf1iDo9RGZrN6D","data":[],"key":"vvRetCsiIK"}],"key":"TQMtUiNO8B"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import functions\nfrom importlib import reload\nreload(functions)\nresult = functions.predict_on_array(\n    ds,\n    model,\n    output_tensor_dim=output_tensor_dim,\n    new_dim=new_dim,\n    core_dim=core_dim,\n    resample_dim=resample_dim,\n    batch_size=4\n)","key":"ZeSNU7I3xM"},{"type":"output","id":"Brh6pJlKOX1Ci9f2oitRX","data":[],"key":"swT8c2dx4l"}],"key":"sJ1IHtRV9N"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%writefile -a test_predict_on_array.py\n\n@pytest.mark.parametrize(\"factor, mode, expected\", [\n    (2.0, \"edges\", np.arange(0, 10, 0.5)),\n    (0.5, \"edges\", np.arange(0, 10, 2.0)),\n])\ndef test_resample_coordinate(factor, mode, expected):\n    coord = xr.DataArray(np.arange(10, dtype=float), dims=\"x\")\n    resampled = _resample_coordinate(coord, factor, mode)\n    np.testing.assert_allclose(resampled, expected)","key":"acTS9Bj3zt"},{"type":"output","id":"vfBCVp8lil1WGLTiOGIpT","data":[],"key":"gMz1qye7KL"}],"key":"QtMSodIn28"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%writefile -a test_predict_on_array.py\n\n@pytest.mark.parametrize(\n    \"model, output_tensor_dim, new_dim, core_dim, resample_dim, manual_transform\",\n    [\n        # Case 1: Identity - No change\n        (\n            Identity(),\n            {'x': 10, 'y': 5},\n            [], [], ['x', 'y'],\n            lambda da: da.data\n        ),\n        # Case 2: ExpandAlongAxis - Upsampling\n        (\n            ExpandAlongAxis(ax=1, n_repeats=2), # ax=1 is 'x'\n            {'x': 20, 'y': 5},\n            [], [], ['x', 'y'],\n            lambda da: da.data.repeat(2, axis=0) # axis=0 in the 2D numpy array\n        ),\n        # Case 3: SubsetAlongAxis - Coarsening\n        (\n            SubsetAlongAxis(ax=1, n=5), # ax=1 is 'x'\n            {'x': 5, 'y': 5},\n            [], [], ['x', 'y'],\n            lambda da: da.isel(x=slice(0, 5)).data\n        ),\n        # Case 4: MeanAlongDim - Dimension reduction\n        (\n            MeanAlongDim(ax=2), # ax=2 is 'y'\n            {'x': 10},\n            [], [], ['x'],\n            lambda da: da.mean(dim='y').data\n        ),\n        # Case 5: AddAxis - Add a new dimension\n        (\n            AddAxis(ax=1), # Add new dim at axis 1\n            {'channel': 1, 'x': 10, 'y': 5},\n            ['channel'], [], ['x', 'y'],\n            lambda da: np.expand_dims(da.data, axis=0)\n        ),\n    ]\n)\ndef test_predict_on_array_all_models(\n    map_dataset_fixture, model, output_tensor_dim, new_dim, core_dim, resample_dim, manual_transform\n):\n    \"\"\"\n    Tests reassembly, averaging, and coordinate assignment using a variety of models.\n    \"\"\"\n    dataset = map_dataset_fixture\n    bgen = dataset.X_generator\n    resample_factor = _get_resample_factor(bgen, output_tensor_dim, resample_dim)\n\n    # --- Run the function under test ---\n    result_da = predict_on_array(\n        dataset=dataset, model=model, output_tensor_dim=output_tensor_dim,\n        new_dim=new_dim, core_dim=core_dim, resample_dim=resample_dim, batch_size=4\n    )\n\n    # --- Manually calculate the expected result ---\n    expected_size = _get_output_array_size(bgen, output_tensor_dim, new_dim, core_dim, resample_dim)\n    expected_sum = xr.DataArray(np.zeros(list(expected_size.values())), dims=list(expected_size.keys()))\n    expected_count = xr.full_like(expected_sum, 0, dtype=int)\n\n    for i in range(len(dataset)):\n        batch_da = bgen[i]\n        old_indexer = bgen._batch_selectors.selectors[i][0]\n        new_indexer = {}\n        for key in old_indexer:\n            if key in resample_dim:\n                new_indexer[key] = slice(int(old_indexer[key].start * resample_factor.get(key, 1)), int(old_indexer[key].stop * resample_factor.get(key, 1)))\n            elif key in core_dim:\n                new_indexer[key] = old_indexer[key]\n\n        model_output_on_batch = manual_transform(batch_da)\n        print(f\"Batch {i}: {new_indexer} -> {model_output_on_batch.shape}\")\n        print(f\"Expected sum shape: {expected_sum.loc[new_indexer].shape}\")\n        expected_sum.loc[new_indexer] += model_output_on_batch\n        expected_count.loc[new_indexer] += 1\n        \n    expected_avg_data = expected_sum.data / expected_count.data\n    \n    # --- Assert correctness ---\n    np.testing.assert_allclose(result_da.values, expected_avg_data, equal_nan=True)","key":"qN1tXW6fMx"},{"type":"output","id":"bGkj14x3g96pfoz1Am_Nq","data":[],"key":"Eky6J9UASB"}],"key":"cpNDplBEQ3"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"!pytest -v test_predict_on_array.py","key":"dSaN8gqgCU"},{"type":"output","id":"gOEETUZQ-fJzZ1kRIGTIA","data":[],"key":"pl7s1jKU9b"}],"key":"Fx7qBdoA3h"}],"key":"qFLBKFUE2G"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"How to Cite This Cookbook","url":"/notebooks/how-to-cite","group":"Preamble"},"next":{"title":"Using xbatcher to train an autoencoder","url":"/notebooks/autoencoder","group":"Example with topography data"}}},"domain":"http://localhost:3000"}